"""
Kaggle ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³è©³ç´°æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹

Kaggle APIã§ã¯å–å¾—ã§ããªã„è©³ç´°æƒ…å ±ã‚’Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—
Playwright ã‚’ä½¿ç”¨ã—ã¦ JavaScript ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾Œã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
"""

from playwright.sync_api import sync_playwright, Page, Browser
from bs4 import BeautifulSoup
from typing import Optional, Dict, Any
from datetime import datetime
import time

from .cache_service import get_cache_service


class ScraperService:
    """Kaggle ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆPlaywrightä½¿ç”¨ï¼‰"""

    def __init__(self, cache_ttl_days: int = 1, headless: bool = True):
        """
        åˆæœŸåŒ–

        Args:
            cache_ttl_days: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé™ï¼ˆæ—¥æ•°ï¼‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1æ—¥
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹
        """
        self.cache_service = get_cache_service()
        self.cache_ttl_days = cache_ttl_days
        self.base_url = "https://www.kaggle.com/competitions"
        self.headless = headless

    def get_competition_details(
        self,
        comp_id: str,
        force_refresh: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è©³ç´°æƒ…å ±ã‚’å–å¾—

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            è©³ç´°æƒ…å ±ã®è¾æ›¸ï¼ˆå–å¾—å¤±æ•—æ™‚ã¯ Noneï¼‰
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(comp_id)
            if cached_data:
                return cached_data

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        print(f"ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {comp_id}")

        try:
            scraped_data = self._scrape_competition(comp_id)

            if scraped_data:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.cache_service.set_scraped_data(
                    comp_id,
                    scraped_data,
                    ttl_days=self.cache_ttl_days
                )
                return scraped_data
            else:
                print(f"âš ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {comp_id}")
                return None

        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({comp_id}): {e}")
            return None

    def _scrape_competition(self, comp_id: str) -> Optional[Dict[str, Any]]:
        """
        å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ï¼ˆPlaywrightä½¿ç”¨ï¼‰

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID

        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        """
        url = f"{self.base_url}/{comp_id}"

        try:
            with sync_playwright() as p:
                # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
                browser = p.chromium.launch(headless=self.headless)
                page = browser.new_page()

                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ30ç§’ï¼‰
                response = page.goto(url, wait_until='networkidle', timeout=30000)

                # 404ãƒã‚§ãƒƒã‚¯
                if response and response.status == 404:
                    print(f"âŒ ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {comp_id}")
                    browser.close()
                    return None

                # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã‚’å¾…æ©Ÿ
                page.wait_for_load_state('networkidle')
                time.sleep(2)  # è¿½åŠ ã®å®‰å…¨å¾…æ©Ÿ

                # ãƒšãƒ¼ã‚¸ã®ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é ˜åŸŸã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                # ã‚ˆã‚Šç°¡æ½”ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: HTMLãƒ‘ãƒ¼ã‚¹ã›ãšã«ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥å–å¾—
                page_text = page.inner_text('#site-content')
                browser.close()

                # çµæœã‚’è¿”ã™ï¼ˆLLMã§å‡¦ç†ã™ã‚‹ãŸã‚ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                result = {
                    'comp_id': comp_id,
                    'url': url,
                    'scraped_at': datetime.now().isoformat(),
                    'full_text': page_text,  # LLMå‡¦ç†ç”¨ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆ
                }

                print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {comp_id} ({len(page_text)} æ–‡å­—)")
                return result

        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({comp_id}): {e}")
            return None


    def get_tab_content(
        self,
        comp_id: str,
        tab: str = "",
        force_refresh: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        æŒ‡å®šã—ãŸã‚¿ãƒ–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
            tab: ã‚¿ãƒ–åï¼ˆ'data', 'discussion', 'code', 'leaderboard'ï¼‰
                 ç©ºæ–‡å­—åˆ—ã®å ´åˆã¯ Overview ã‚¿ãƒ–
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            ã‚¿ãƒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¾æ›¸ï¼ˆå–å¾—å¤±æ•—æ™‚ã¯ Noneï¼‰
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã«ã‚¿ãƒ–åã‚’å«ã‚ã‚‹
        cache_key = f"{comp_id}:{tab}" if tab else comp_id

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                return cached_data

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        print(f"ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {comp_id}/{tab or 'overview'}")

        try:
            # URLæ§‹ç¯‰
            url = f"{self.base_url}/{comp_id}/{tab}" if tab else f"{self.base_url}/{comp_id}"

            with sync_playwright() as p:
                # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
                browser = p.chromium.launch(headless=self.headless)
                page = browser.new_page()

                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•
                response = page.goto(url, wait_until='networkidle', timeout=30000)

                # 404ãƒã‚§ãƒƒã‚¯
                if response and response.status == 404:
                    print(f"âŒ ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {comp_id}/{tab or 'overview'}")
                    browser.close()
                    return None

                # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã‚’å¾…æ©Ÿ
                page.wait_for_load_state('networkidle')
                time.sleep(2)  # è¿½åŠ ã®å®‰å…¨å¾…æ©Ÿ

                # ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                page_text = page.inner_text('#site-content')
                browser.close()

                # çµæœã‚’ä½œæˆ
                result = {
                    'comp_id': comp_id,
                    'tab': tab or 'overview',
                    'url': url,
                    'scraped_at': datetime.now().isoformat(),
                    'full_text': page_text,
                }

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.cache_service.set_scraped_data(
                    cache_key,
                    result,
                    ttl_days=self.cache_ttl_days
                )

                print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {comp_id}/{tab or 'overview'} ({len(page_text)} æ–‡å­—)")
                return result

        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({comp_id}/{tab or 'overview'}): {e}")
            return None

    def _get_author_tier_from_item(self, item, author_name: str) -> Optional[str]:
        """
        ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ å†…ã‹ã‚‰ç§°å·ã‚’ç›´æ¥æ¢ã™

        Args:
            item: ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®locator
            author_name: æŠ•ç¨¿è€…å

        Returns:
            ç§°å·ï¼ˆGrandmaster, Master, Expert, Contributor, Noviceï¼‰ã¾ãŸã¯None
        """
        try:
            # ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            item_text = item.inner_text()

            # ãƒ†ã‚­ã‚¹ãƒˆå†…ã‹ã‚‰ç§°å·ã‚’æ¢ã™
            tiers = ['Grandmaster', 'Master', 'Expert', 'Contributor', 'Novice']
            for tier in tiers:
                if tier.lower() in item_text.lower():
                    print(f"      â†’ ã‚¢ã‚¤ãƒ†ãƒ å†…ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç§°å·æ¤œå‡º: {tier}")
                    return tier

            # HTMLã‹ã‚‰ç§°å·ãƒãƒƒã‚¸ã‚’æ¢ã™ï¼ˆimgã®altå±æ€§ã‚„aria-labelãªã©ï¼‰
            tier_badges = item.locator('img[alt*="tier"], [aria-label*="tier"], [title*="Grandmaster"], [title*="Master"]').all()
            for badge in tier_badges:
                alt_text = badge.get_attribute('alt') or ''
                aria_label = badge.get_attribute('aria-label') or ''
                title = badge.get_attribute('title') or ''

                combined_text = f"{alt_text} {aria_label} {title}".lower()

                for tier in tiers:
                    if tier.lower() in combined_text:
                        print(f"      â†’ ãƒãƒƒã‚¸ã‹ã‚‰ç§°å·æ¤œå‡º: {tier}")
                        return tier

            # SVGã‚¢ã‚¤ã‚³ãƒ³ã‚’æ¢ã™
            svg_icons = item.locator('svg').all()
            for svg in svg_icons:
                aria_label = svg.get_attribute('aria-label') or ''
                title = svg.get_attribute('title') or ''

                combined_text = f"{aria_label} {title}".lower()

                for tier in tiers:
                    if tier.lower() in combined_text:
                        print(f"      â†’ SVGã‹ã‚‰ç§°å·æ¤œå‡º: {tier}")
                        return tier

        except Exception as e:
            print(f"      ç§°å·æ¤œå‡ºã‚¨ãƒ©ãƒ¼ (from item): {e}")

        return None

    def _get_tier_color_from_item(self, item) -> Optional[str]:
        """
        ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ å†…ã‹ã‚‰SVG circleã®stroke colorã‚’æŠ½å‡º

        Args:
            item: ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®locator

        Returns:
            RGBè‰²æ–‡å­—åˆ—ï¼ˆä¾‹: "rgb(235, 204, 41)"ï¼‰ã¾ãŸã¯None
        """
        try:
            # SVGè¦ç´ ã‚’æ¢ã™
            svg_elements = item.locator('svg').all()

            for svg in svg_elements:
                # SVGå†…ã®circleè¦ç´ ã‚’æ¢ã™ï¼ˆè¤‡æ•°ã‚ã‚‹å ´åˆã¯2ç•ªç›®ã‚’å–å¾—ï¼‰
                circles = svg.locator('circle').all()

                if len(circles) >= 2:
                    # 2ç•ªç›®ã®circleã‹ã‚‰stroke colorã‚’å–å¾—
                    second_circle = circles[1]
                    style_attr = second_circle.get_attribute('style')

                    if style_attr and 'stroke:' in style_attr:
                        # styleå±æ€§ã‹ã‚‰ stroke: rgb(...) ã‚’æŠ½å‡º
                        import re
                        match = re.search(r'stroke:\s*(rgb\([^)]+\))', style_attr)
                        if match:
                            color = match.group(1)
                            print(f"      â†’ SVG circleè‰²æ¤œå‡º: {color}")
                            return color

        except Exception as e:
            print(f"      SVGè‰²æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return None

    def _get_author_tier(self, page: Page, author_link_locator) -> Optional[str]:
        """
        æŠ•ç¨¿è€…ã«ãƒ›ãƒãƒ¼ã—ã¦ç§°å·ï¼ˆtierï¼‰ã‚’å–å¾—

        Args:
            page: Playwrightã®ãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            author_link_locator: æŠ•ç¨¿è€…ãƒªãƒ³ã‚¯ã®locator

        Returns:
            ç§°å·ï¼ˆGrandmaster, Master, Expert, Contributor, Noviceï¼‰ã¾ãŸã¯None
        """
        try:
            # ãƒã‚¦ã‚¹ã‚ªãƒ¼ãƒãƒ¼
            author_link_locator.hover(timeout=5000)
            time.sleep(2)  # ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—è¡¨ç¤ºã‚’å¾…æ©Ÿï¼ˆå¢—ã‚„ã—ãŸï¼‰

            # è¤‡æ•°ã®æ–¹æ³•ã§ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ã‚’æ¢ã™
            # æ–¹æ³•1: role="tooltip"
            tooltip = page.locator('[role="tooltip"]').first
            if tooltip.count() > 0:
                tooltip_text = tooltip.text_content(timeout=3000)
                print(f"      [tooltip] æ¤œå‡º: {tooltip_text[:100]}")

                # ç§°å·ã‚’æ¢ã™ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
                tiers = ['Grandmaster', 'Master', 'Expert', 'Contributor', 'Novice']
                for tier in tiers:
                    if tier.lower() in tooltip_text.lower():
                        print(f"      â†’ ç§°å·æ¤œå‡º: {tier}")
                        return tier

            # æ–¹æ³•2: MuiTooltipã‚’æ¢ã™
            mui_tooltip = page.locator('.MuiTooltip-tooltip').first
            if mui_tooltip.count() > 0:
                tooltip_text = mui_tooltip.text_content(timeout=3000)
                print(f"      [MuiTooltip] æ¤œå‡º: {tooltip_text[:100]}")

                tiers = ['Grandmaster', 'Master', 'Expert', 'Contributor', 'Novice']
                for tier in tiers:
                    if tier.lower() in tooltip_text.lower():
                        print(f"      â†’ ç§°å·æ¤œå‡º: {tier}")
                        return tier

            # æ–¹æ³•3: data-testid="tooltip"
            testid_tooltip = page.locator('[data-testid="tooltip"]').first
            if testid_tooltip.count() > 0:
                tooltip_text = testid_tooltip.text_content(timeout=3000)
                print(f"      [testid] æ¤œå‡º: {tooltip_text[:100]}")

                tiers = ['Grandmaster', 'Master', 'Expert', 'Contributor', 'Novice']
                for tier in tiers:
                    if tier.lower() in tooltip_text.lower():
                        print(f"      â†’ ç§°å·æ¤œå‡º: {tier}")
                        return tier

            print(f"      ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—æœªæ¤œå‡º")

            # ãƒ›ãƒãƒ¼ã‚’è§£é™¤
            page.mouse.move(0, 0)
            time.sleep(0.5)

        except Exception as e:
            print(f"      ç§°å·å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

        return None

    def get_discussions(
        self,
        comp_id: str,
        max_pages: int = 1,
        force_refresh: bool = False
    ) -> Optional[list[Dict[str, Any]]]:
        """
        ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—ï¼ˆDiscussions + Writeups ã®ä¸¡æ–¹ï¼‰

        Kaggleã¯è‡ªå‹•çš„ã«æŠ•ç¥¨æ•°é †ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€
        1ãƒšãƒ¼ã‚¸ç›®ã‚’å–å¾—ã™ã‚Œã°æœ€ã‚‚é‡è¦ãªãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ãŒå¾—ã‚‰ã‚Œã‚‹

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
            max_pages: å–å¾—ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1ãƒšãƒ¼ã‚¸ï¼‰
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã®ãƒªã‚¹ãƒˆï¼ˆDiscussions + Writeupsï¼‰
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        cache_key = f"{comp_id}:discussions:p{max_pages}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {comp_id} discussions")
                return cached_data.get('discussions', cached_data)

        base_url = f"{self.base_url}/{comp_id}/discussion?sort=votes"

        # Discussions ã‚¿ãƒ–ã¨ Writeups ã‚¿ãƒ–ã®ä¸¡æ–¹ã‚’å–å¾—
        tabs = [
            ('discussion', base_url),
            ('writeup', f"{base_url}&tab=writeups")
        ]

        try:
            with sync_playwright() as p:
                browser: Browser = p.chromium.launch(headless=self.headless)
                page: Page = browser.new_page()

                all_discussions = []
                seen_urls = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨

                # å„ã‚¿ãƒ–ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                for tab_type, tab_url in tabs:
                    print(f"\nğŸ“‹ {tab_type.upper()} ã‚¿ãƒ–ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")

                    for page_num in range(1, max_pages + 1):
                        page_url = f"{tab_url}&page={page_num}" if page_num > 1 else tab_url

                        page.goto(page_url, wait_until="networkidle", timeout=30000)
                        page.wait_for_timeout(2000)

                        # Playwrightã®locator APIã‚’ä½¿ç”¨
                        discussion_items = page.locator('li.MuiListItem-root').all()

                        if not discussion_items:
                            print(f"  ãƒšãƒ¼ã‚¸{page_num}: ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            break

                        print(f"  ãƒšãƒ¼ã‚¸{page_num}: {len(discussion_items)}ä»¶ã®ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚’å‡¦ç†ä¸­...")

                        for idx, item in enumerate(discussion_items, 1):
                            try:
                                # ã‚¿ã‚¤ãƒˆãƒ«ã¨URLï¼ˆ/discussion/ ã¾ãŸã¯ /writeups/ ã®ä¸¡æ–¹ã«å¯¾å¿œï¼‰
                                title_link = item.locator('a[href*="/competitions/"]').first
                                if title_link.count() == 0:
                                    continue

                                raw_title = title_link.text_content(timeout=3000).strip()
                                href = title_link.get_attribute('href')
                                discussion_url = f"https://www.kaggle.com{href}" if href.startswith('/') else href

                                # URLã‹ã‚‰ category ã‚’åˆ¤å®š
                                category = 'writeup' if '/writeups/' in discussion_url else 'discussion'

                                # æŠ•ç¨¿è€…æƒ…å ±
                                author = None
                                author_tier = None
                                tier_color = None
                                # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼ˆaria-labelã«'s profileã‚’å«ã‚€ã‚‚ã®ï¼‰
                                author_links = item.locator('a[aria-label*="profile"]').all()

                                for author_link in author_links:
                                    link_href = author_link.get_attribute('href')
                                    aria_label = author_link.get_attribute('aria-label')

                                    if aria_label and "'s profile" in aria_label:
                                        author = aria_label.split("'s profile")[0]
                                        print(f"    [{idx}] Author: {author}")

                                        # ç§°å·ã‚’å–å¾—ï¼ˆæ–°ã—ã„æ–¹æ³•ï¼šãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ å†…ã‚’æ¢ã™ï¼‰
                                        author_tier = self._get_author_tier_from_item(item, author)
                                        if author_tier:
                                            print(f"    [{idx}] {author}: {author_tier}")

                                        # ç§°å·è‰²ã‚’å–å¾—
                                        tier_color = self._get_tier_color_from_item(item)
                                        if tier_color:
                                            print(f"    [{idx}] Tier color: {tier_color}")
                                        break

                                # æŠ•ç¥¨æ•°
                                vote_count = 0
                                vote_locator = item.locator('span[aria-label*="vote"]').first
                                if vote_locator.count() > 0:
                                    vote_label = vote_locator.get_attribute('aria-label')
                                    if vote_label:
                                        try:
                                            vote_count = int(vote_label.split()[0])
                                        except (ValueError, IndexError):
                                            pass

                                # ã‚³ãƒ¡ãƒ³ãƒˆæ•°
                                comment_count = 0
                                comment_locators = item.locator('span').all()
                                for comment_loc in comment_locators:
                                    text = comment_loc.text_content()
                                    if text and 'comment' in text.lower():
                                        try:
                                            comment_count = int(text.split()[0])
                                            break
                                        except (ValueError, IndexError):
                                            pass

                                # ãƒ”ãƒ³ç•™ã‚ãƒã‚§ãƒƒã‚¯ - ãƒ”ãƒ³ç•™ã‚ã¯é™¤å¤–
                                is_pinned = item.locator('text=push_pin').count() > 0
                                if is_pinned:
                                    continue  # Pinned topicsã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›ï¼‰

                                # ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                                # ãƒ‘ã‚¿ãƒ¼ãƒ³: "[Title][Author] Â· Last comment..." or "[Title][Author]"
                                title = raw_title

                                # " Â· Last comment..." ä»¥é™ã‚’å‰Šé™¤
                                if ' Â· Last comment' in title:
                                    title = title.split(' Â· Last comment')[0]

                                # æœ«å°¾ã®ä½œè€…åã‚’å‰Šé™¤ï¼ˆä½œè€…åãŒå–å¾—ã§ãã¦ã„ã‚‹å ´åˆï¼‰
                                if author and title.endswith(author):
                                    title = title[:-len(author)].strip()

                                # ãã®ä»–ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå¿µã®ãŸã‚ï¼‰
                                title = title.strip()

                                # é‡è¤‡ãƒã‚§ãƒƒã‚¯: URLãŒæ—¢ã«è¿½åŠ æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                                if discussion_url in seen_urls:
                                    continue

                                seen_urls.add(discussion_url)
                                all_discussions.append({
                                    'title': title,
                                    'url': discussion_url,
                                    'author': author,
                                    'author_tier': author_tier,
                                    'tier_color': tier_color,
                                    'vote_count': vote_count,
                                    'comment_count': comment_count,
                                    'category': category,  # URLã‹ã‚‰åˆ¤å®šã—ãŸ category ã‚’è¨­å®š
                                    'is_pinned': False,  # ãƒ”ãƒ³ç•™ã‚ã¯é™¤å¤–ã—ã¦ã„ã‚‹ã®ã§å¸¸ã«False
                                })

                            except Exception as e:
                                print(f"    ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ è§£æã‚¨ãƒ©ãƒ¼ [{idx}]: {e}")
                                continue

                browser.close()

                print(f"\nå–å¾—å®Œäº†: {len(all_discussions)}ä»¶")

                # æŠ•ç¥¨æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆKaggleã®ãƒšãƒ¼ã‚¸ã¨åŒã˜é †åºã‚’ç¶­æŒï¼‰
                sorted_discussions = sorted(all_discussions, key=lambda x: x['vote_count'], reverse=True)

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if sorted_discussions:
                    result = {
                        'comp_id': comp_id,
                        'max_pages': max_pages,
                        'scraped_at': datetime.now().isoformat(),
                        'discussions': sorted_discussions
                    }
                    self.cache_service.set_scraped_data(
                        cache_key,
                        result,
                        ttl_days=self.cache_ttl_days
                    )

                print(f"âœ“ {len(sorted_discussions)}ä»¶ã®ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                return sorted_discussions

        except Exception as e:
            print(f"âœ— ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— ({comp_id} discussions): {e}")
            import traceback
            traceback.print_exc()
            return None

    def get_notebooks(
        self,
        comp_id: str,
        max_pages: int = 1,
        force_refresh: bool = False
    ) -> Optional[list[Dict[str, Any]]]:
        """
        ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä¸€è¦§ã‚’å–å¾—ï¼ˆCodeã‚¿ãƒ–ï¼‰

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
            max_pages: å–å¾—ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1ãƒšãƒ¼ã‚¸ï¼‰
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        cache_key = f"{comp_id}:notebooks:p{max_pages}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {comp_id} notebooks")
                return cached_data.get('notebooks', cached_data)

        base_url = f"{self.base_url}/{comp_id}/code?sortBy=voteCount"

        try:
            with sync_playwright() as p:
                browser: Browser = p.chromium.launch(headless=self.headless)
                page: Page = browser.new_page()

                all_notebooks = []
                seen_urls = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨

                print(f"\nğŸ“” CODE ã‚¿ãƒ–ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")

                for page_num in range(1, max_pages + 1):
                    page_url = f"{base_url}&page={page_num}" if page_num > 1 else base_url

                    page.goto(page_url, wait_until="networkidle", timeout=30000)
                    page.wait_for_timeout(2000)

                    # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¢ã™ï¼ˆå®Ÿéš›ã®HTMLæ§‹é€ ã«åˆã‚ã›ã‚‹ï¼‰
                    # 'km-listitem--large' ã‚¯ãƒ©ã‚¹ã‚’æŒã¤divè¦ç´ 
                    notebook_items = page.locator('div.km-listitem--large').all()

                    if not notebook_items:
                        print(f"  ãƒšãƒ¼ã‚¸{page_num}: ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        break

                    print(f"  ãƒšãƒ¼ã‚¸{page_num}: {len(notebook_items)}ä»¶ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å‡¦ç†ä¸­...")

                    for idx, item in enumerate(notebook_items, 1):
                        try:
                            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ï¼ˆaria-labelå±æ€§ã‹ã‚‰ï¼‰
                            title_link = item.locator('a[aria-label][role="link"]').first
                            if title_link.count() == 0:
                                continue

                            title = title_link.get_attribute('aria-label')
                            if not title:
                                continue

                            # URLã‚’ã‚³ãƒ¡ãƒ³ãƒˆãƒªãƒ³ã‚¯ã‹ã‚‰æ§‹ç¯‰
                            comment_link = item.locator('a[href*="/comments"]').first
                            if comment_link.count() == 0:
                                continue

                            href = comment_link.get_attribute('href')
                            if not href:
                                continue

                            # /code/username/notebook-name/comments â†’ /code/username/notebook-name
                            notebook_url = href.replace('/comments', '')
                            notebook_url = f"https://www.kaggle.com{notebook_url}" if notebook_url.startswith('/') else notebook_url

                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                            if notebook_url in seen_urls:
                                continue

                            seen_urls.add(notebook_url)

                            # æŠ•ç¨¿è€…æƒ…å ±
                            author = None
                            author_tier = None
                            tier_color = None

                            # ä½œæˆè€…ãƒªãƒ³ã‚¯: aria-label ã« "profile" ã‚’å«ã‚€
                            author_links = item.locator('a[aria-label*="profile"]').all()
                            for author_link in author_links:
                                aria_label = author_link.get_attribute('aria-label')
                                if aria_label and "'s profile" in aria_label:
                                    author = aria_label.split("'s profile")[0]
                                    print(f"    [{idx}] Author: {author}")

                                    # ç§°å·ã‚’å–å¾—ï¼ˆTierãƒãƒƒã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯SVGè‰²ã‹ã‚‰ï¼‰
                                    # "Gold", "Silver", "Bronze" ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
                                    tier_spans = item.locator('span:has-text("Gold"), span:has-text("Silver"), span:has-text("Bronze"), span:has-text("Expert"), span:has-text("Master"), span:has-text("Grandmaster")').all()
                                    for tier_span in tier_spans:
                                        tier_text = tier_span.text_content().strip()
                                        if tier_text:
                                            author_tier = tier_text
                                            print(f"    [{idx}] {author}: {author_tier}")
                                            break

                                    # ç§°å·è‰²ã‚’å–å¾—
                                    tier_color = self._get_tier_color_from_item(item)
                                    if tier_color:
                                        print(f"    [{idx}] Tier color: {tier_color}")
                                    break

                            # æŠ•ç¥¨æ•°ï¼ˆaria-label="N votes"ï¼‰
                            vote_count = 0
                            vote_locator = item.locator('span[aria-label*="vote"]').first
                            if vote_locator.count() > 0:
                                vote_label = vote_locator.get_attribute('aria-label')
                                if vote_label:
                                    try:
                                        # "1246 votes" â†’ 1246
                                        vote_count = int(vote_label.split()[0])
                                    except (ValueError, IndexError):
                                        pass

                            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ï¼ˆ"89 comments"ã®ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆï¼‰
                            comment_count = 0
                            if comment_link.count() > 0:
                                comment_text = comment_link.text_content()
                                if comment_text and 'comment' in comment_text.lower():
                                    try:
                                        # "89 comments" â†’ 89
                                        comment_count = int(comment_text.split()[0])
                                    except (ValueError, IndexError):
                                        pass

                            all_notebooks.append({
                                'title': title,
                                'url': notebook_url,
                                'author': author,
                                'author_tier': author_tier,
                                'tier_color': tier_color,
                                'vote_count': vote_count,
                                'comment_count': comment_count,
                                'type': 'notebook'
                            })

                            print(f"    [{idx}] âœ“ {title[:50]}... (votes={vote_count}, comments={comment_count})")

                        except Exception as e:
                            print(f"    ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚¢ã‚¤ãƒ†ãƒ è§£æã‚¨ãƒ©ãƒ¼ [{idx}]: {e}")
                            continue

                browser.close()

                print(f"\nå–å¾—å®Œäº†: {len(all_notebooks)}ä»¶")

                # æŠ•ç¥¨æ•°ã§ã‚½ãƒ¼ãƒˆ
                sorted_notebooks = sorted(all_notebooks, key=lambda x: x['vote_count'], reverse=True)

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if sorted_notebooks:
                    result = {
                        'comp_id': comp_id,
                        'max_pages': max_pages,
                        'scraped_at': datetime.now().isoformat(),
                        'notebooks': sorted_notebooks
                    }
                    self.cache_service.set_scraped_data(
                        cache_key,
                        result,
                        ttl_days=self.cache_ttl_days
                    )

                print(f"âœ“ {len(sorted_notebooks)}ä»¶ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                return sorted_notebooks

        except Exception as e:
            print(f"âœ— ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— ({comp_id} notebooks): {e}")
            import traceback
            traceback.print_exc()
            return None

    def get_discussion_detail(
        self,
        discussion_url: str,
        force_refresh: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        å€‹åˆ¥ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã®è©³ç´°ã‚’å–å¾—

        Args:
            discussion_url: ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã®URL
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã®è¾æ›¸ï¼ˆå–å¾—å¤±æ•—æ™‚ã¯ Noneï¼‰
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ï¼ˆURLã®æœ€å¾Œã®éƒ¨åˆ†ã‚’ä½¿ç”¨ï¼‰
        discussion_id = discussion_url.split('/')[-1].split('#')[0]
        cache_key = f"discussion:{discussion_id}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {discussion_id}")
                return cached_data

        print(f"ğŸŒ ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {discussion_id}")

        try:
            with sync_playwright() as p:
                browser: Browser = p.chromium.launch(headless=self.headless)
                page: Page = browser.new_page()

                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•
                response = page.goto(discussion_url, wait_until="networkidle", timeout=30000)

                # 404ãƒã‚§ãƒƒã‚¯
                if response and response.status == 404:
                    print(f"âŒ ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {discussion_url}")
                    browser.close()
                    return None

                # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã‚’å¾…æ©Ÿ
                page.wait_for_load_state('networkidle')
                time.sleep(2)

                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
                content_text = page.inner_text('#site-content')
                browser.close()

                # çµæœã‚’ä½œæˆ
                result = {
                    'discussion_id': discussion_id,
                    'url': discussion_url,
                    'scraped_at': datetime.now().isoformat(),
                    'content': content_text,
                }

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.cache_service.set_scraped_data(
                    cache_key,
                    result,
                    ttl_days=self.cache_ttl_days
                )

                print(f"âœ“ å–å¾—å®Œäº†: {discussion_id} ({len(content_text)} æ–‡å­—)")
                return result

        except Exception as e:
            print(f"âœ— ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— ({discussion_id}): {e}")
            import traceback
            traceback.print_exc()
            return None

    def get_writeups(
        self,
        comp_id: str,
        max_pages: int = 3,
        force_refresh: bool = False
    ) -> Optional[list[Dict[str, Any]]]:
        """
        ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®Writeupsï¼ˆå…¬å¼è§£æ³•æŠ•ç¨¿ï¼‰ä¸€è¦§ã‚’å–å¾—

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
            max_pages: å–å¾—ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ãƒšãƒ¼ã‚¸ï¼‰
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            Writeupsæƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        cache_key = f"{comp_id}:writeups:p{max_pages}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {comp_id} writeups")
                return cached_data.get('writeups', cached_data)

        url = f"{self.base_url}/{comp_id}/writeups"
        print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {url}")

        try:
            with sync_playwright() as p:
                browser: Browser = p.chromium.launch(headless=self.headless)
                page: Page = browser.new_page()

                all_writeups = []

                for page_num in range(1, max_pages + 1):
                    page_url = f"{url}?page={page_num}" if page_num > 1 else url

                    response = page.goto(page_url, wait_until="networkidle", timeout=30000)

                    # 404ãƒã‚§ãƒƒã‚¯ï¼ˆWriteupsãƒšãƒ¼ã‚¸ãŒãªã„å ´åˆï¼‰
                    if response and response.status == 404:
                        print(f"  Writeupsãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚³ãƒ³ãƒšãŒå¤ã„å¯èƒ½æ€§ï¼‰")
                        break

                    page.wait_for_timeout(2000)

                    # Playwrightã®locator APIã‚’ä½¿ç”¨ï¼ˆDiscussionsã¨åŒã˜æ§‹é€ ï¼‰
                    writeup_items = page.locator('li.MuiListItem-root').all()

                    if not writeup_items:
                        print(f"  ãƒšãƒ¼ã‚¸{page_num}: WriteupsãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        break

                    print(f"  ãƒšãƒ¼ã‚¸{page_num}: {len(writeup_items)}ä»¶ã®Writeupsã‚’å‡¦ç†ä¸­...")

                    for idx, item in enumerate(writeup_items, 1):
                        try:
                            # ã‚¿ã‚¤ãƒˆãƒ«ã¨URLï¼ˆ/writeups/ ã‚’å«ã‚€ãƒªãƒ³ã‚¯ï¼‰
                            title_link = item.locator('a[href*="/writeups/"]').first
                            if title_link.count() == 0:
                                continue

                            raw_title = title_link.text_content(timeout=3000).strip()
                            href = title_link.get_attribute('href')
                            writeup_url = f"https://www.kaggle.com{href}" if href.startswith('/') else href

                            # æŠ•ç¨¿è€…æƒ…å ±
                            author = None
                            author_tier = None
                            tier_color = None
                            author_links = item.locator('a[aria-label*="profile"]').all()

                            for author_link in author_links:
                                aria_label = author_link.get_attribute('aria-label')
                                if aria_label and "'s profile" in aria_label:
                                    author = aria_label.split("'s profile")[0]
                                    print(f"    [{idx}] Author: {author}")

                                    # ç§°å·ã‚’å–å¾—
                                    author_tier = self._get_author_tier_from_item(item, author)
                                    if author_tier:
                                        print(f"    [{idx}] {author}: {author_tier}")

                                    # ç§°å·è‰²ã‚’å–å¾—
                                    tier_color = self._get_tier_color_from_item(item)
                                    if tier_color:
                                        print(f"    [{idx}] Tier color: {tier_color}")
                                    break

                            # æŠ•ç¥¨æ•°
                            vote_count = 0
                            vote_locator = item.locator('span[aria-label*="vote"]').first
                            if vote_locator.count() > 0:
                                vote_label = vote_locator.get_attribute('aria-label')
                                if vote_label:
                                    try:
                                        vote_count = int(vote_label.split()[0])
                                    except (ValueError, IndexError):
                                        pass

                            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°
                            comment_count = 0
                            comment_locators = item.locator('span').all()
                            for comment_loc in comment_locators:
                                text = comment_loc.text_content()
                                if text and 'comment' in text.lower():
                                    try:
                                        comment_count = int(text.split()[0])
                                        break
                                    except (ValueError, IndexError):
                                        pass

                            # ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                            title = raw_title
                            if ' Â· Last comment' in title:
                                title = title.split(' Â· Last comment')[0]
                            if author and title.endswith(author):
                                title = title[:-len(author)].strip()
                            title = title.strip()

                            all_writeups.append({
                                'title': title,
                                'url': writeup_url,
                                'author': author,
                                'author_tier': author_tier,
                                'tier_color': tier_color,
                                'vote_count': vote_count,
                                'comment_count': comment_count,
                                'category': 'writeup',
                                'is_pinned': False,
                            })

                        except Exception as e:
                            print(f"    Writeupã‚¢ã‚¤ãƒ†ãƒ è§£æã‚¨ãƒ©ãƒ¼ [{idx}]: {e}")
                            continue

                browser.close()

                print(f"\nå–å¾—å®Œäº†: {len(all_writeups)}ä»¶")

                # æŠ•ç¥¨æ•°ã§ã‚½ãƒ¼ãƒˆ
                sorted_writeups = sorted(all_writeups, key=lambda x: x['vote_count'], reverse=True)

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if sorted_writeups:
                    result = {
                        'comp_id': comp_id,
                        'max_pages': max_pages,
                        'scraped_at': datetime.now().isoformat(),
                        'writeups': sorted_writeups
                    }
                    self.cache_service.set_scraped_data(
                        cache_key,
                        result,
                        ttl_days=self.cache_ttl_days
                    )

                print(f"âœ“ {len(sorted_writeups)}ä»¶ã®Writeupsã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                return sorted_writeups

        except Exception as e:
            print(f"âœ— Writeupsã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— ({comp_id}): {e}")
            import traceback
            traceback.print_exc()
            return None

    def scrape_competition_metadata(
        self,
        comp_id: str,
        force_refresh: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        ã‚³ãƒ³ãƒšãƒšãƒ¼ã‚¸ã‹ã‚‰æ§‹é€ åŒ–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ID
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¾æ›¸ï¼ˆid, title, description, start_date, end_date, status, metricï¼‰
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        cache_key = f"comp_metadata:{comp_id}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                return cached_data

        url = f"{self.base_url}/{comp_id}"
        print(f"ğŸŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—: {comp_id}")

        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=self.headless)
                page = browser.new_page()

                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•
                response = page.goto(url, wait_until='networkidle', timeout=30000)

                # 404ãƒã‚§ãƒƒã‚¯
                if response and response.status == 404:
                    print(f"âŒ ã‚³ãƒ³ãƒšãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {comp_id}")
                    browser.close()
                    return None

                page.wait_for_load_state('networkidle')
                time.sleep(2)

                # HTMLã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹
                html = page.content()
                soup = BeautifulSoup(html, 'html.parser')

                # 1. ã‚¿ã‚¤ãƒˆãƒ«
                title = None
                h1 = soup.find('h1')
                if h1:
                    title = h1.get_text().strip()

                # 2. èª¬æ˜æ–‡ï¼ˆæœ€åˆã®æ®µè½ï¼‰
                description = None
                p_tags = soup.find_all('p')
                for p in p_tags:
                    text = p.get_text().strip()
                    if len(text) > 50:  # ååˆ†ãªé•·ã•ã®æ®µè½ã‚’æ¢ã™
                        description = text
                        break

                # 3. å…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ—¥ä»˜ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æŠ½å‡º
                full_text = page.inner_text('body')
                lines = [line.strip() for line in full_text.split('\n')]

                start_date = None
                end_date = None
                status = None
                metric = None

                for line in lines:
                    line_lower = line.lower()

                    # é–‹å§‹æ—¥
                    if not start_date and 'started' in line_lower:
                        # "Started 2 months ago" ã®ã‚ˆã†ãªå½¢å¼
                        start_date = line

                    # çµ‚äº†æ—¥ãƒ»ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                    if not end_date:
                        if 'ended' in line_lower or 'closed' in line_lower:
                            end_date = line
                            status = 'completed'
                        elif 'closes' in line_lower or 'deadline' in line_lower:
                            end_date = line
                            status = 'active'

                    # è©•ä¾¡æŒ‡æ¨™
                    if not metric and ('evaluation' in line_lower or 'metric' in line_lower):
                        # æ¬¡ã®è¡Œã¾ãŸã¯åŒã˜è¡Œã‹ã‚‰æŒ‡æ¨™åã‚’æŠ½å‡º
                        if len(line) < 100:  # çŸ­ã„è¡Œãªã‚‰æŒ‡æ¨™ã®å¯èƒ½æ€§
                            metric = line

                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒåˆ¤å®šã§ããªã„å ´åˆã¯endedã‹ã‚‰æ¨æ¸¬
                if not status:
                    if end_date and ('ended' in end_date.lower() or 'closed' in end_date.lower()):
                        status = 'completed'
                    else:
                        status = 'active'

                browser.close()

                # çµæœã‚’ä½œæˆ
                result = {
                    'id': comp_id,
                    'title': title,
                    'url': url,
                    'description': description,
                    'start_date': start_date,
                    'end_date': end_date,
                    'status': status,
                    'metric': metric,
                    'scraped_at': datetime.now().isoformat(),
                }

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.cache_service.set_scraped_data(
                    cache_key,
                    result,
                    ttl_days=self.cache_ttl_days
                )

                print(f"âœ“ {comp_id}: {title}")
                return result

        except Exception as e:
            print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ ({comp_id}): {e}")
            import traceback
            traceback.print_exc()
            return None

    def scrape_competitions_list(
        self,
        max_pages: int = 10,
        prestige_filter: str = "medals",
        participation_filter: str = "open",
        force_refresh: bool = False,
        include_details: bool = False
    ) -> list[str] | list[Dict[str, Any]]:
        """
        Kaggleã‚³ãƒ³ãƒšä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚³ãƒ³ãƒšIDã®ãƒªã‚¹ãƒˆã‚’å–å¾—

        Args:
            max_pages: å–å¾—ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
            prestige_filter: prestigeãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆ"medals", "all"ãªã©ï¼‰
            participation_filter: participationãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆ"open", "all"ãªã©ï¼‰
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—
            include_details: ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æ¦‚è¦ã‚‚å«ã‚ã¦è¿”ã™ã‹

        Returns:
            include_details=False: ã‚³ãƒ³ãƒšIDã®ãƒªã‚¹ãƒˆ
            include_details=True: è©³ç´°æƒ…å ±ã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆ
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        cache_key = f"competitions_list:{prestige_filter}:{participation_filter}:p{max_pages}"
        if include_details:
            cache_key += ':details'

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                if include_details:
                    comps = cached_data.get('competitions', [])
                    print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {len(comps)}ä»¶ã®ã‚³ãƒ³ãƒšè©³ç´°")
                    return comps
                else:
                    comp_ids = cached_data.get('competition_ids', [])
                    print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {len(comp_ids)}ä»¶ã®ã‚³ãƒ³ãƒšID")
                    return comp_ids

        print(f"ğŸ“ ã‚³ãƒ³ãƒšä¸€è¦§ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ (æœ€å¤§{max_pages}ãƒšãƒ¼ã‚¸)...")

        # include_detailsã«ã‚ˆã£ã¦åˆæœŸå€¤ã‚’å¤‰ãˆã‚‹
        if include_details:
            all_comp_ids = []  # ãƒªã‚¹ãƒˆ
        else:
            all_comp_ids = set()  # ã‚»ãƒƒãƒˆ

        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=self.headless)
                page = browser.new_page()

                for page_num in range(1, max_pages + 1):
                    # URLæ§‹ç¯‰
                    url = f"{self.base_url}?prestigeFilter={prestige_filter}&participationFilter={participation_filter}&page={page_num}"

                    try:
                        page.goto(url, wait_until='networkidle', timeout=60000)
                        time.sleep(2)

                        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ­ãƒ¼ãƒ‰
                        for i in range(3):
                            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                            time.sleep(0.3)

                        # HTMLã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹
                        html = page.content()
                        soup = BeautifulSoup(html, 'html.parser')

                        if include_details:
                            # è©³ç´°æƒ…å ±ä»˜ãã§å–å¾—
                            # ã‚³ãƒ³ãƒšã‚«ãƒ¼ãƒ‰ã‚’æ¢ã™ï¼ˆå„ã‚³ãƒ³ãƒšã¯ç‰¹å®šã®divæ§‹é€ å†…ã«ã‚ã‚‹ï¼‰
                            comp_cards = soup.find_all('div', class_=lambda x: x and 'sc-kSaXSp' in str(x))

                            page_comps_detailed = []
                            for card in comp_cards:
                                # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæœ€åˆã®divã¾ãŸã¯spanï¼‰
                                title_elem = card.find('div', class_=lambda x: x and 'sc-kCuUfV' in str(x))
                                if not title_elem:
                                    title_elem = card.find('span', class_=lambda x: x and 'sc-kCuUfV' in str(x))

                                title = title_elem.get_text().strip() if title_elem else None

                                # æ¦‚è¦ï¼ˆèª¬æ˜æ–‡ã®spanï¼‰
                                desc_spans = card.find_all('span', class_=lambda x: x and 'sc-eqNDNG' in str(x) and 'sc-fYRIQK' in str(x))
                                description = None
                                for span in desc_spans:
                                    text = span.get_text().strip()
                                    # "Featured Â· Code Competition" ã®ã‚ˆã†ãªè¡Œã¯é™¤å¤–
                                    if text and 'Â·' not in text and len(text) > 20:
                                        description = text
                                        break

                                # ã‚³ãƒ³ãƒšIDã‚’ãƒªãƒ³ã‚¯ã‹ã‚‰å–å¾—
                                link = card.find_parent('a', href=lambda x: x and '/competitions/' in str(x))
                                if not link:
                                    # ã‚«ãƒ¼ãƒ‰å†…ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                                    link = card.find('a', href=lambda x: x and '/competitions/' in str(x))

                                if link:
                                    href = link.get('href', '')
                                    comp_id = href.replace('/competitions/', '').split('/')[0].split('?')[0]

                                    if comp_id and title:
                                        page_comps_detailed.append({
                                            'id': comp_id,
                                            'title': title,
                                            'description': description or '',
                                            'url': f"https://www.kaggle.com/competitions/{comp_id}"
                                        })

                            all_comp_ids.extend(page_comps_detailed)
                            print(f"   ãƒšãƒ¼ã‚¸ {page_num:2d}: {len(page_comps_detailed):2d}ä»¶ (åˆè¨ˆ: {len(all_comp_ids)}ä»¶)")

                            if len(page_comps_detailed) == 0:
                                print(f"   ãƒšãƒ¼ã‚¸ {page_num} ã§ãƒ‡ãƒ¼ã‚¿ãªã—ã€çµ‚äº†")
                                break
                        else:
                            # IDã®ã¿å–å¾—ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
                            comp_links = soup.find_all('a', href=lambda x: x and x.startswith('/competitions/') and x != '/competitions')
                            page_comps = set()
                            for link in comp_links:
                                href = link['href']
                                comp_id = href.replace('/competitions/', '').split('/')[0].split('?')[0]
                                if comp_id:
                                    page_comps.add(comp_id)

                            all_comp_ids.update(page_comps)
                            print(f"   ãƒšãƒ¼ã‚¸ {page_num:2d}: {len(page_comps):2d}ä»¶ (åˆè¨ˆ: {len(all_comp_ids)}ä»¶)")

                            if len(page_comps) == 0:
                                print(f"   ãƒšãƒ¼ã‚¸ {page_num} ã§ãƒ‡ãƒ¼ã‚¿ãªã—ã€çµ‚äº†")
                                break

                    except Exception as e:
                        print(f"   âš ï¸ ãƒšãƒ¼ã‚¸ {page_num} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                        break

                browser.close()

            if include_details:
                # è©³ç´°æƒ…å ±ä»˜ããƒªã‚¹ãƒˆ
                # é‡è¤‡ã‚’é™¤å»ï¼ˆIDã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰
                seen_ids = set()
                unique_comps = []
                for comp in all_comp_ids:
                    if comp['id'] not in seen_ids:
                        seen_ids.add(comp['id'])
                        unique_comps.append(comp)

                # IDã§ã‚½ãƒ¼ãƒˆ
                unique_comps.sort(key=lambda x: x['id'])

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if unique_comps:
                    result = {
                        'competitions': unique_comps,
                        'scraped_at': datetime.now().isoformat(),
                        'max_pages': max_pages,
                        'prestige_filter': prestige_filter,
                        'participation_filter': participation_filter,
                    }
                    self.cache_service.set_scraped_data(
                        cache_key,
                        result,
                        ttl_days=self.cache_ttl_days
                    )

                print(f"\nâœ… åˆè¨ˆ {len(unique_comps)}ä»¶ã®ã‚³ãƒ³ãƒšè©³ç´°ã‚’å–å¾—ã—ã¾ã—ãŸ")
                return unique_comps
            else:
                # IDã®ã¿ã®ãƒªã‚¹ãƒˆ
                comp_ids_list = sorted(list(all_comp_ids))

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if comp_ids_list:
                    result = {
                        'competition_ids': comp_ids_list,
                        'scraped_at': datetime.now().isoformat(),
                        'max_pages': max_pages,
                        'prestige_filter': prestige_filter,
                        'participation_filter': participation_filter,
                    }
                    self.cache_service.set_scraped_data(
                        cache_key,
                        result,
                        ttl_days=self.cache_ttl_days
                    )

                print(f"\nâœ… åˆè¨ˆ {len(comp_ids_list)}ä»¶ã®ã‚³ãƒ³ãƒšIDã‚’å–å¾—ã—ã¾ã—ãŸ")
                return comp_ids_list

        except Exception as e:
            print(f"âŒ ã‚³ãƒ³ãƒšä¸€è¦§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []

    def scrape_multiple(
        self,
        comp_ids: list[str],
        delay_seconds: float = 2.0
    ) -> Dict[str, Optional[Dict[str, Any]]]:
        """
        è¤‡æ•°ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼‰

        Args:
            comp_ids: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID ã®ãƒªã‚¹ãƒˆ
            delay_seconds: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰

        Returns:
            {comp_id: scraped_data} ã®è¾æ›¸
        """
        results = {}

        for i, comp_id in enumerate(comp_ids):
            print(f"\n[{i+1}/{len(comp_ids)}] å‡¦ç†ä¸­: {comp_id}")

            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            data = self.get_competition_details(comp_id)
            results[comp_id] = data

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆæœ€å¾Œã®1ä»¶ä»¥å¤–ï¼‰
            if i < len(comp_ids) - 1:
                print(f"â³ {delay_seconds}ç§’å¾…æ©Ÿ...")
                time.sleep(delay_seconds)

        return results


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
_scraper_service_instance = None


def get_scraper_service(cache_ttl_days: int = 1) -> ScraperService:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _scraper_service_instance
    if _scraper_service_instance is None:
        _scraper_service_instance = ScraperService(cache_ttl_days=cache_ttl_days)
    return _scraper_service_instance

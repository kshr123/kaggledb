"""
Kaggle ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³è©³ç´°æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹

Kaggle APIã§ã¯å–å¾—ã§ããªã„è©³ç´°æƒ…å ±ã‚’Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—
Playwright ã‚’ä½¿ç”¨ã—ã¦ JavaScript ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾Œã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
"""

from playwright.sync_api import sync_playwright, Page, Browser
from bs4 import BeautifulSoup
from typing import Optional, Dict, Any
from datetime import datetime
import time

from .cache_service import get_cache_service


class ScraperService:
    """Kaggle ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆPlaywrightä½¿ç”¨ï¼‰"""

    def __init__(self, cache_ttl_days: int = 1, headless: bool = True):
        """
        åˆæœŸåŒ–

        Args:
            cache_ttl_days: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé™ï¼ˆæ—¥æ•°ï¼‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1æ—¥
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹
        """
        self.cache_service = get_cache_service()
        self.cache_ttl_days = cache_ttl_days
        self.base_url = "https://www.kaggle.com/competitions"
        self.headless = headless

    def get_competition_details(
        self,
        comp_id: str,
        force_refresh: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è©³ç´°æƒ…å ±ã‚’å–å¾—

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            è©³ç´°æƒ…å ±ã®è¾æ›¸ï¼ˆå–å¾—å¤±æ•—æ™‚ã¯ Noneï¼‰
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(comp_id)
            if cached_data:
                return cached_data

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        print(f"ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {comp_id}")

        try:
            scraped_data = self._scrape_competition(comp_id)

            if scraped_data:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.cache_service.set_scraped_data(
                    comp_id,
                    scraped_data,
                    ttl_days=self.cache_ttl_days
                )
                return scraped_data
            else:
                print(f"âš ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {comp_id}")
                return None

        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({comp_id}): {e}")
            return None

    def _scrape_competition(self, comp_id: str) -> Optional[Dict[str, Any]]:
        """
        å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ï¼ˆPlaywrightä½¿ç”¨ï¼‰

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID

        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        """
        url = f"{self.base_url}/{comp_id}"

        try:
            with sync_playwright() as p:
                # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
                browser = p.chromium.launch(headless=self.headless)
                page = browser.new_page()

                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ30ç§’ï¼‰
                response = page.goto(url, wait_until='networkidle', timeout=30000)

                # 404ãƒã‚§ãƒƒã‚¯
                if response and response.status == 404:
                    print(f"âŒ ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {comp_id}")
                    browser.close()
                    return None

                # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã‚’å¾…æ©Ÿ
                page.wait_for_load_state('networkidle')
                time.sleep(2)  # è¿½åŠ ã®å®‰å…¨å¾…æ©Ÿ

                # ãƒšãƒ¼ã‚¸ã®ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é ˜åŸŸã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                # ã‚ˆã‚Šç°¡æ½”ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: HTMLãƒ‘ãƒ¼ã‚¹ã›ãšã«ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥å–å¾—
                page_text = page.inner_text('#site-content')
                browser.close()

                # çµæœã‚’è¿”ã™ï¼ˆLLMã§å‡¦ç†ã™ã‚‹ãŸã‚ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                result = {
                    'comp_id': comp_id,
                    'url': url,
                    'scraped_at': datetime.now().isoformat(),
                    'full_text': page_text,  # LLMå‡¦ç†ç”¨ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆ
                }

                print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {comp_id} ({len(page_text)} æ–‡å­—)")
                return result

        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({comp_id}): {e}")
            return None


    def scrape_multiple(
        self,
        comp_ids: list[str],
        delay_seconds: float = 2.0
    ) -> Dict[str, Optional[Dict[str, Any]]]:
        """
        è¤‡æ•°ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼‰

        Args:
            comp_ids: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID ã®ãƒªã‚¹ãƒˆ
            delay_seconds: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰

        Returns:
            {comp_id: scraped_data} ã®è¾æ›¸
        """
        results = {}

        for i, comp_id in enumerate(comp_ids):
            print(f"\n[{i+1}/{len(comp_ids)}] å‡¦ç†ä¸­: {comp_id}")

            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            data = self.get_competition_details(comp_id)
            results[comp_id] = data

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆæœ€å¾Œã®1ä»¶ä»¥å¤–ï¼‰
            if i < len(comp_ids) - 1:
                print(f"â³ {delay_seconds}ç§’å¾…æ©Ÿ...")
                time.sleep(delay_seconds)

        return results


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
_scraper_service_instance = None


def get_scraper_service(cache_ttl_days: int = 1) -> ScraperService:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _scraper_service_instance
    if _scraper_service_instance is None:
        _scraper_service_instance = ScraperService(cache_ttl_days=cache_ttl_days)
    return _scraper_service_instance

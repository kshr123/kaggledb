"""
Kaggle ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³è©³ç´°æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹

Kaggle APIã§ã¯å–å¾—ã§ããªã„è©³ç´°æƒ…å ±ã‚’Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—
Playwright ã‚’ä½¿ç”¨ã—ã¦ JavaScript ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾Œã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
"""

from playwright.sync_api import sync_playwright, Page, Browser
from bs4 import BeautifulSoup
from typing import Optional, Dict, Any
from datetime import datetime
import time

from .cache_service import get_cache_service


class ScraperService:
    """Kaggle ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆPlaywrightä½¿ç”¨ï¼‰"""

    def __init__(self, cache_ttl_days: int = 1, headless: bool = True):
        """
        åˆæœŸåŒ–

        Args:
            cache_ttl_days: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé™ï¼ˆæ—¥æ•°ï¼‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1æ—¥
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹
        """
        self.cache_service = get_cache_service()
        self.cache_ttl_days = cache_ttl_days
        self.base_url = "https://www.kaggle.com/competitions"
        self.headless = headless

    def get_competition_details(
        self,
        comp_id: str,
        force_refresh: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è©³ç´°æƒ…å ±ã‚’å–å¾—

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            è©³ç´°æƒ…å ±ã®è¾æ›¸ï¼ˆå–å¾—å¤±æ•—æ™‚ã¯ Noneï¼‰
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(comp_id)
            if cached_data:
                return cached_data

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        print(f"ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {comp_id}")

        try:
            scraped_data = self._scrape_competition(comp_id)

            if scraped_data:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.cache_service.set_scraped_data(
                    comp_id,
                    scraped_data,
                    ttl_days=self.cache_ttl_days
                )
                return scraped_data
            else:
                print(f"âš ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {comp_id}")
                return None

        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({comp_id}): {e}")
            return None

    def _scrape_competition(self, comp_id: str) -> Optional[Dict[str, Any]]:
        """
        å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ï¼ˆPlaywrightä½¿ç”¨ï¼‰

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID

        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        """
        url = f"{self.base_url}/{comp_id}"

        try:
            with sync_playwright() as p:
                # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
                browser = p.chromium.launch(headless=self.headless)
                page = browser.new_page()

                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ30ç§’ï¼‰
                response = page.goto(url, wait_until='networkidle', timeout=30000)

                # 404ãƒã‚§ãƒƒã‚¯
                if response and response.status == 404:
                    print(f"âŒ ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {comp_id}")
                    browser.close()
                    return None

                # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã‚’å¾…æ©Ÿ
                page.wait_for_load_state('networkidle')
                time.sleep(2)  # è¿½åŠ ã®å®‰å…¨å¾…æ©Ÿ

                # ãƒšãƒ¼ã‚¸ã®ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é ˜åŸŸã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                # ã‚ˆã‚Šç°¡æ½”ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: HTMLãƒ‘ãƒ¼ã‚¹ã›ãšã«ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥å–å¾—
                page_text = page.inner_text('#site-content')
                browser.close()

                # çµæœã‚’è¿”ã™ï¼ˆLLMã§å‡¦ç†ã™ã‚‹ãŸã‚ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                result = {
                    'comp_id': comp_id,
                    'url': url,
                    'scraped_at': datetime.now().isoformat(),
                    'full_text': page_text,  # LLMå‡¦ç†ç”¨ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆ
                }

                print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {comp_id} ({len(page_text)} æ–‡å­—)")
                return result

        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({comp_id}): {e}")
            return None


    def get_tab_content(
        self,
        comp_id: str,
        tab: str = "",
        force_refresh: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        æŒ‡å®šã—ãŸã‚¿ãƒ–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
            tab: ã‚¿ãƒ–åï¼ˆ'data', 'discussion', 'code', 'leaderboard'ï¼‰
                 ç©ºæ–‡å­—åˆ—ã®å ´åˆã¯ Overview ã‚¿ãƒ–
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            ã‚¿ãƒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¾æ›¸ï¼ˆå–å¾—å¤±æ•—æ™‚ã¯ Noneï¼‰
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã«ã‚¿ãƒ–åã‚’å«ã‚ã‚‹
        cache_key = f"{comp_id}:{tab}" if tab else comp_id

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                return cached_data

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        print(f"ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {comp_id}/{tab or 'overview'}")

        try:
            # URLæ§‹ç¯‰
            url = f"{self.base_url}/{comp_id}/{tab}" if tab else f"{self.base_url}/{comp_id}"

            with sync_playwright() as p:
                # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
                browser = p.chromium.launch(headless=self.headless)
                page = browser.new_page()

                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•
                response = page.goto(url, wait_until='networkidle', timeout=30000)

                # 404ãƒã‚§ãƒƒã‚¯
                if response and response.status == 404:
                    print(f"âŒ ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {comp_id}/{tab or 'overview'}")
                    browser.close()
                    return None

                # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã‚’å¾…æ©Ÿ
                page.wait_for_load_state('networkidle')
                time.sleep(2)  # è¿½åŠ ã®å®‰å…¨å¾…æ©Ÿ

                # ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                page_text = page.inner_text('#site-content')
                browser.close()

                # çµæœã‚’ä½œæˆ
                result = {
                    'comp_id': comp_id,
                    'tab': tab or 'overview',
                    'url': url,
                    'scraped_at': datetime.now().isoformat(),
                    'full_text': page_text,
                }

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.cache_service.set_scraped_data(
                    cache_key,
                    result,
                    ttl_days=self.cache_ttl_days
                )

                print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {comp_id}/{tab or 'overview'} ({len(page_text)} æ–‡å­—)")
                return result

        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({comp_id}/{tab or 'overview'}): {e}")
            return None

    def _get_author_tier_from_item(self, item, author_name: str) -> Optional[str]:
        """
        ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ å†…ã‹ã‚‰ç§°å·ã‚’ç›´æ¥æ¢ã™

        Args:
            item: ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®locator
            author_name: æŠ•ç¨¿è€…å

        Returns:
            ç§°å·ï¼ˆGrandmaster, Master, Expert, Contributor, Noviceï¼‰ã¾ãŸã¯None
        """
        try:
            # ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            item_text = item.inner_text()

            # ãƒ†ã‚­ã‚¹ãƒˆå†…ã‹ã‚‰ç§°å·ã‚’æ¢ã™
            tiers = ['Grandmaster', 'Master', 'Expert', 'Contributor', 'Novice']
            for tier in tiers:
                if tier.lower() in item_text.lower():
                    print(f"      â†’ ã‚¢ã‚¤ãƒ†ãƒ å†…ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç§°å·æ¤œå‡º: {tier}")
                    return tier

            # HTMLã‹ã‚‰ç§°å·ãƒãƒƒã‚¸ã‚’æ¢ã™ï¼ˆimgã®altå±æ€§ã‚„aria-labelãªã©ï¼‰
            tier_badges = item.locator('img[alt*="tier"], [aria-label*="tier"], [title*="Grandmaster"], [title*="Master"]').all()
            for badge in tier_badges:
                alt_text = badge.get_attribute('alt') or ''
                aria_label = badge.get_attribute('aria-label') or ''
                title = badge.get_attribute('title') or ''

                combined_text = f"{alt_text} {aria_label} {title}".lower()

                for tier in tiers:
                    if tier.lower() in combined_text:
                        print(f"      â†’ ãƒãƒƒã‚¸ã‹ã‚‰ç§°å·æ¤œå‡º: {tier}")
                        return tier

            # SVGã‚¢ã‚¤ã‚³ãƒ³ã‚’æ¢ã™
            svg_icons = item.locator('svg').all()
            for svg in svg_icons:
                aria_label = svg.get_attribute('aria-label') or ''
                title = svg.get_attribute('title') or ''

                combined_text = f"{aria_label} {title}".lower()

                for tier in tiers:
                    if tier.lower() in combined_text:
                        print(f"      â†’ SVGã‹ã‚‰ç§°å·æ¤œå‡º: {tier}")
                        return tier

        except Exception as e:
            print(f"      ç§°å·æ¤œå‡ºã‚¨ãƒ©ãƒ¼ (from item): {e}")

        return None

    def _get_tier_color_from_item(self, item) -> Optional[str]:
        """
        ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ å†…ã‹ã‚‰SVG circleã®stroke colorã‚’æŠ½å‡º

        Args:
            item: ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®locator

        Returns:
            RGBè‰²æ–‡å­—åˆ—ï¼ˆä¾‹: "rgb(235, 204, 41)"ï¼‰ã¾ãŸã¯None
        """
        try:
            # SVGè¦ç´ ã‚’æ¢ã™
            svg_elements = item.locator('svg').all()

            for svg in svg_elements:
                # SVGå†…ã®circleè¦ç´ ã‚’æ¢ã™ï¼ˆè¤‡æ•°ã‚ã‚‹å ´åˆã¯2ç•ªç›®ã‚’å–å¾—ï¼‰
                circles = svg.locator('circle').all()

                if len(circles) >= 2:
                    # 2ç•ªç›®ã®circleã‹ã‚‰stroke colorã‚’å–å¾—
                    second_circle = circles[1]
                    style_attr = second_circle.get_attribute('style')

                    if style_attr and 'stroke:' in style_attr:
                        # styleå±æ€§ã‹ã‚‰ stroke: rgb(...) ã‚’æŠ½å‡º
                        import re
                        match = re.search(r'stroke:\s*(rgb\([^)]+\))', style_attr)
                        if match:
                            color = match.group(1)
                            print(f"      â†’ SVG circleè‰²æ¤œå‡º: {color}")
                            return color

        except Exception as e:
            print(f"      SVGè‰²æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return None

    def _get_author_tier(self, page: Page, author_link_locator) -> Optional[str]:
        """
        æŠ•ç¨¿è€…ã«ãƒ›ãƒãƒ¼ã—ã¦ç§°å·ï¼ˆtierï¼‰ã‚’å–å¾—

        Args:
            page: Playwrightã®ãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            author_link_locator: æŠ•ç¨¿è€…ãƒªãƒ³ã‚¯ã®locator

        Returns:
            ç§°å·ï¼ˆGrandmaster, Master, Expert, Contributor, Noviceï¼‰ã¾ãŸã¯None
        """
        try:
            # ãƒã‚¦ã‚¹ã‚ªãƒ¼ãƒãƒ¼
            author_link_locator.hover(timeout=5000)
            time.sleep(2)  # ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—è¡¨ç¤ºã‚’å¾…æ©Ÿï¼ˆå¢—ã‚„ã—ãŸï¼‰

            # è¤‡æ•°ã®æ–¹æ³•ã§ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ã‚’æ¢ã™
            # æ–¹æ³•1: role="tooltip"
            tooltip = page.locator('[role="tooltip"]').first
            if tooltip.count() > 0:
                tooltip_text = tooltip.text_content(timeout=3000)
                print(f"      [tooltip] æ¤œå‡º: {tooltip_text[:100]}")

                # ç§°å·ã‚’æ¢ã™ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
                tiers = ['Grandmaster', 'Master', 'Expert', 'Contributor', 'Novice']
                for tier in tiers:
                    if tier.lower() in tooltip_text.lower():
                        print(f"      â†’ ç§°å·æ¤œå‡º: {tier}")
                        return tier

            # æ–¹æ³•2: MuiTooltipã‚’æ¢ã™
            mui_tooltip = page.locator('.MuiTooltip-tooltip').first
            if mui_tooltip.count() > 0:
                tooltip_text = mui_tooltip.text_content(timeout=3000)
                print(f"      [MuiTooltip] æ¤œå‡º: {tooltip_text[:100]}")

                tiers = ['Grandmaster', 'Master', 'Expert', 'Contributor', 'Novice']
                for tier in tiers:
                    if tier.lower() in tooltip_text.lower():
                        print(f"      â†’ ç§°å·æ¤œå‡º: {tier}")
                        return tier

            # æ–¹æ³•3: data-testid="tooltip"
            testid_tooltip = page.locator('[data-testid="tooltip"]').first
            if testid_tooltip.count() > 0:
                tooltip_text = testid_tooltip.text_content(timeout=3000)
                print(f"      [testid] æ¤œå‡º: {tooltip_text[:100]}")

                tiers = ['Grandmaster', 'Master', 'Expert', 'Contributor', 'Novice']
                for tier in tiers:
                    if tier.lower() in tooltip_text.lower():
                        print(f"      â†’ ç§°å·æ¤œå‡º: {tier}")
                        return tier

            print(f"      ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—æœªæ¤œå‡º")

            # ãƒ›ãƒãƒ¼ã‚’è§£é™¤
            page.mouse.move(0, 0)
            time.sleep(0.5)

        except Exception as e:
            print(f"      ç§°å·å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

        return None

    def get_discussions(
        self,
        comp_id: str,
        max_pages: int = 1,
        force_refresh: bool = False
    ) -> Optional[list[Dict[str, Any]]]:
        """
        ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—ï¼ˆ1ãƒšãƒ¼ã‚¸åˆ†ã®ä¸Šä½ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ï¼‰

        Kaggleã¯è‡ªå‹•çš„ã«æŠ•ç¥¨æ•°é †ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€
        1ãƒšãƒ¼ã‚¸ç›®ã‚’å–å¾—ã™ã‚Œã°æœ€ã‚‚é‡è¦ãªãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ãŒå¾—ã‚‰ã‚Œã‚‹

        Args:
            comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
            max_pages: å–å¾—ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1ãƒšãƒ¼ã‚¸ï¼‰
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        cache_key = f"{comp_id}:discussions:p{max_pages}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {comp_id} discussions")
                return cached_data.get('discussions', cached_data)

        url = f"{self.base_url}/{comp_id}/discussion?sort=votes"
        print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {url}")

        try:
            with sync_playwright() as p:
                browser: Browser = p.chromium.launch(headless=self.headless)
                page: Page = browser.new_page()

                all_discussions = []

                for page_num in range(1, max_pages + 1):
                    page_url = f"{url}&page={page_num}" if page_num > 1 else url

                    page.goto(page_url, wait_until="networkidle", timeout=30000)
                    page.wait_for_timeout(2000)

                    # Playwrightã®locator APIã‚’ä½¿ç”¨
                    discussion_items = page.locator('li.MuiListItem-root').all()

                    if not discussion_items:
                        print(f"  ãƒšãƒ¼ã‚¸{page_num}: ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        break

                    print(f"  ãƒšãƒ¼ã‚¸{page_num}: {len(discussion_items)}ä»¶ã®ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚’å‡¦ç†ä¸­...")

                    for idx, item in enumerate(discussion_items, 1):
                        try:
                            # ã‚¿ã‚¤ãƒˆãƒ«ã¨URL
                            title_link = item.locator('a[href*="/discussion/"]').first
                            if title_link.count() == 0:
                                continue

                            title = title_link.text_content(timeout=3000).strip()
                            href = title_link.get_attribute('href')
                            discussion_url = f"https://www.kaggle.com{href}" if href.startswith('/') else href

                            # æŠ•ç¨¿è€…æƒ…å ±
                            author = None
                            author_tier = None
                            tier_color = None
                            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼ˆaria-labelã«'s profileã‚’å«ã‚€ã‚‚ã®ï¼‰
                            author_links = item.locator('a[aria-label*="profile"]').all()

                            for author_link in author_links:
                                link_href = author_link.get_attribute('href')
                                aria_label = author_link.get_attribute('aria-label')

                                if aria_label and "'s profile" in aria_label:
                                    author = aria_label.split("'s profile")[0]
                                    print(f"    [{idx}] Author: {author}")

                                    # ç§°å·ã‚’å–å¾—ï¼ˆæ–°ã—ã„æ–¹æ³•ï¼šãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ å†…ã‚’æ¢ã™ï¼‰
                                    author_tier = self._get_author_tier_from_item(item, author)
                                    if author_tier:
                                        print(f"    [{idx}] {author}: {author_tier}")

                                    # ç§°å·è‰²ã‚’å–å¾—
                                    tier_color = self._get_tier_color_from_item(item)
                                    if tier_color:
                                        print(f"    [{idx}] Tier color: {tier_color}")
                                    break

                            # æŠ•ç¥¨æ•°
                            vote_count = 0
                            vote_locator = item.locator('span[aria-label*="vote"]').first
                            if vote_locator.count() > 0:
                                vote_label = vote_locator.get_attribute('aria-label')
                                if vote_label:
                                    try:
                                        vote_count = int(vote_label.split()[0])
                                    except (ValueError, IndexError):
                                        pass

                            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°
                            comment_count = 0
                            comment_locators = item.locator('span').all()
                            for comment_loc in comment_locators:
                                text = comment_loc.text_content()
                                if text and 'comment' in text.lower():
                                    try:
                                        comment_count = int(text.split()[0])
                                        break
                                    except (ValueError, IndexError):
                                        pass

                            # ãƒ”ãƒ³ç•™ã‚ãƒã‚§ãƒƒã‚¯ - ãƒ”ãƒ³ç•™ã‚ã¯é™¤å¤–
                            is_pinned = item.locator('text=push_pin').count() > 0
                            if is_pinned:
                                continue  # Pinned topicsã¯ã‚¹ã‚­ãƒƒãƒ—

                            all_discussions.append({
                                'title': title,
                                'url': discussion_url,
                                'author': author,
                                'author_tier': author_tier,
                                'tier_color': tier_color,
                                'vote_count': vote_count,
                                'comment_count': comment_count,
                                'category': None,
                                'is_pinned': False,  # ãƒ”ãƒ³ç•™ã‚ã¯é™¤å¤–ã—ã¦ã„ã‚‹ã®ã§å¸¸ã«False
                            })

                        except Exception as e:
                            print(f"    ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ è§£æã‚¨ãƒ©ãƒ¼ [{idx}]: {e}")
                            continue

                browser.close()

                print(f"\nå–å¾—å®Œäº†: {len(all_discussions)}ä»¶")

                # æŠ•ç¥¨æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆKaggleã®ãƒšãƒ¼ã‚¸ã¨åŒã˜é †åºã‚’ç¶­æŒï¼‰
                sorted_discussions = sorted(all_discussions, key=lambda x: x['vote_count'], reverse=True)

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if sorted_discussions:
                    result = {
                        'comp_id': comp_id,
                        'max_pages': max_pages,
                        'scraped_at': datetime.now().isoformat(),
                        'discussions': sorted_discussions
                    }
                    self.cache_service.set_scraped_data(
                        cache_key,
                        result,
                        ttl_days=self.cache_ttl_days
                    )

                print(f"âœ“ {len(sorted_discussions)}ä»¶ã®ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                return sorted_discussions

        except Exception as e:
            print(f"âœ— ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— ({comp_id} discussions): {e}")
            import traceback
            traceback.print_exc()
            return None

    def get_discussion_detail(
        self,
        discussion_url: str,
        force_refresh: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        å€‹åˆ¥ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã®è©³ç´°ã‚’å–å¾—

        Args:
            discussion_url: ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã®URL
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†å–å¾—

        Returns:
            ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã®è¾æ›¸ï¼ˆå–å¾—å¤±æ•—æ™‚ã¯ Noneï¼‰
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ï¼ˆURLã®æœ€å¾Œã®éƒ¨åˆ†ã‚’ä½¿ç”¨ï¼‰
        discussion_id = discussion_url.split('/')[-1].split('#')[0]
        cache_key = f"discussion:{discussion_id}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.cache_service.get_scraped_data(cache_key)
            if cached_data:
                print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {discussion_id}")
                return cached_data

        print(f"ğŸŒ ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {discussion_id}")

        try:
            with sync_playwright() as p:
                browser: Browser = p.chromium.launch(headless=self.headless)
                page: Page = browser.new_page()

                # ãƒšãƒ¼ã‚¸ã«ç§»å‹•
                response = page.goto(discussion_url, wait_until="networkidle", timeout=30000)

                # 404ãƒã‚§ãƒƒã‚¯
                if response and response.status == 404:
                    print(f"âŒ ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {discussion_url}")
                    browser.close()
                    return None

                # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã‚’å¾…æ©Ÿ
                page.wait_for_load_state('networkidle')
                time.sleep(2)

                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
                content_text = page.inner_text('#site-content')
                browser.close()

                # çµæœã‚’ä½œæˆ
                result = {
                    'discussion_id': discussion_id,
                    'url': discussion_url,
                    'scraped_at': datetime.now().isoformat(),
                    'content': content_text,
                }

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.cache_service.set_scraped_data(
                    cache_key,
                    result,
                    ttl_days=self.cache_ttl_days
                )

                print(f"âœ“ å–å¾—å®Œäº†: {discussion_id} ({len(content_text)} æ–‡å­—)")
                return result

        except Exception as e:
            print(f"âœ— ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— ({discussion_id}): {e}")
            import traceback
            traceback.print_exc()
            return None

    def scrape_multiple(
        self,
        comp_ids: list[str],
        delay_seconds: float = 2.0
    ) -> Dict[str, Optional[Dict[str, Any]]]:
        """
        è¤‡æ•°ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼‰

        Args:
            comp_ids: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID ã®ãƒªã‚¹ãƒˆ
            delay_seconds: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰

        Returns:
            {comp_id: scraped_data} ã®è¾æ›¸
        """
        results = {}

        for i, comp_id in enumerate(comp_ids):
            print(f"\n[{i+1}/{len(comp_ids)}] å‡¦ç†ä¸­: {comp_id}")

            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            data = self.get_competition_details(comp_id)
            results[comp_id] = data

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆæœ€å¾Œã®1ä»¶ä»¥å¤–ï¼‰
            if i < len(comp_ids) - 1:
                print(f"â³ {delay_seconds}ç§’å¾…æ©Ÿ...")
                time.sleep(delay_seconds)

        return results


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
_scraper_service_instance = None


def get_scraper_service(cache_ttl_days: int = 1) -> ScraperService:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _scraper_service_instance
    if _scraper_service_instance is None:
        _scraper_service_instance = ScraperService(cache_ttl_days=cache_ttl_days)
    return _scraper_service_instance

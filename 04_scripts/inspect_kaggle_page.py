#!/usr/bin/env python3
"""
Kaggle ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã® HTML æ§‹é€ ã‚’èª¿æŸ»ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import requests
from bs4 import BeautifulSoup
import sys


def inspect_page(comp_id: str):
    """
    Kaggle ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã® HTML æ§‹é€ ã‚’èª¿æŸ»

    Args:
        comp_id: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ ID
    """
    url = f"https://www.kaggle.com/competitions/{comp_id}"

    print("=" * 80)
    print(f"Kaggle ãƒšãƒ¼ã‚¸æ§‹é€ èª¿æŸ»: {comp_id}")
    print("=" * 80)
    print(f"URL: {url}\n")

    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml',
    }

    try:
        print("â³ ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        print(f"âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}\n")

        soup = BeautifulSoup(response.text, 'lxml')

        # HTML ã‚’ä¿å­˜
        output_file = f"/tmp/{comp_id}_page.html"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"ğŸ“„ HTML ä¿å­˜: {output_file}\n")

        # ä¸»è¦ãªè¦ç´ ã‚’æ¢ç´¢
        print("=" * 80)
        print("ä¸»è¦ãªè¦ç´ ã®æ¢ç´¢")
        print("=" * 80)

        # ã‚¿ã‚¤ãƒˆãƒ«
        title = soup.find('title')
        if title:
            print(f"\nğŸ“Œ ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«:\n{title.get_text(strip=True)}\n")

        # ã™ã¹ã¦ã® div ã®ã‚¯ãƒ©ã‚¹ã‚’èª¿æŸ»
        print("\nğŸ“¦ div è¦ç´ ã®ã‚¯ãƒ©ã‚¹åï¼ˆæœ€åˆã®50å€‹ï¼‰:")
        print("-" * 80)
        divs = soup.find_all('div', class_=True)
        unique_classes = set()
        for div in divs[:50]:
            classes = ' '.join(div.get('class', []))
            if classes:
                unique_classes.add(classes)

        for cls in sorted(list(unique_classes))[:30]:
            print(f"  - {cls}")

        # id å±æ€§ã‚’æŒã¤è¦ç´ 
        print("\nğŸ†” id å±æ€§ã‚’æŒã¤è¦ç´ ï¼ˆæœ€åˆã®30å€‹ï¼‰:")
        print("-" * 80)
        elements_with_id = soup.find_all(id=True)
        for elem in elements_with_id[:30]:
            print(f"  - <{elem.name} id=\"{elem.get('id')}\">")

        # script ã‚¿ã‚°ï¼ˆJSON ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ï¼‰
        print("\nğŸ“œ script ã‚¿ã‚°ï¼ˆJSON ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ç´¢ï¼‰:")
        print("-" * 80)
        scripts = soup.find_all('script')
        for i, script in enumerate(scripts[:10]):
            script_text = script.get_text()[:200]
            if 'competition' in script_text.lower() or 'description' in script_text.lower():
                print(f"  Script {i+1}: {script_text}...")

        print("\n" + "=" * 80)
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("=" * 80)
        print(f"1. ä¿å­˜ã•ã‚ŒãŸ HTML ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã: {output_file}")
        print("2. ãƒ–ãƒ©ã‚¦ã‚¶ã®é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§å®Ÿéš›ã®ãƒšãƒ¼ã‚¸ã‚’æ¤œè¨¼")
        print("3. æ­£ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ç‰¹å®š")
        print("4. scraper_service.py ã® _extract_* ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä¿®æ­£")

    except requests.RequestException as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Kaggle ãƒšãƒ¼ã‚¸ã® HTML æ§‹é€ ã‚’èª¿æŸ»")
    parser.add_argument(
        "comp_id",
        type=str,
        default="titanic",
        nargs="?",
        help="ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ IDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: titanicï¼‰"
    )

    args = parser.parse_args()
    inspect_page(args.comp_id)

#!/usr/bin/env python3
"""
Data ã‚¿ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Data ã‚¿ãƒ–ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—ã—ã€LLMã§æŠ½å‡ºã™ã‚‹æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ
"""

import sys
import os
import json

# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '02_backend'))

from app.services.scraper_service import get_scraper_service
from app.services.llm_service import get_llm_service


def main():
    print("=" * 60)
    print("Data ã‚¿ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)

    # ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆæœ‰åãªã‚‚ã®ï¼‰
    test_comp_id = "titanic"

    # ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
    try:
        scraper = get_scraper_service()
        llm = get_llm_service()
        print("âœ… ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–å¤±æ•—: {e}")
        return

    print(f"\nğŸ“‹ ãƒ†ã‚¹ãƒˆå¯¾è±¡: {test_comp_id}")
    print("-" * 60)

    # 1. Data ã‚¿ãƒ–ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    print("\nğŸŒ Data ã‚¿ãƒ–ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
    data_tab_content = scraper.get_tab_content(test_comp_id, tab="data")

    if not data_tab_content:
        print("âŒ Data ã‚¿ãƒ–ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—")
        return

    print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {len(data_tab_content.get('full_text', ''))} æ–‡å­—å–å¾—")

    # ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤ºï¼ˆç¢ºèªç”¨ï¼‰
    text_preview = data_tab_content['full_text'][:500]
    print(f"\nğŸ“„ å–å¾—ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
    print(f"{text_preview}...")

    # 2. LLMã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’æŠ½å‡º
    print(f"\nğŸ¤– LLMã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’æŠ½å‡ºä¸­...")
    dataset_info = llm.extract_dataset_info(
        data_text=data_tab_content['full_text'],
        title=test_comp_id
    )

    if dataset_info:
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±æŠ½å‡ºæˆåŠŸ")
        print(f"\nğŸ“Š æŠ½å‡ºçµæœ:")
        print(json.dumps(dataset_info, ensure_ascii=False, indent=2))

        # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è©³ç´°
        print(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ({len(dataset_info.get('files', []))}ä»¶):")
        for file in dataset_info.get('files', []):
            print(f"  - {file}")

        if dataset_info.get('total_size'):
            print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚µã‚¤ã‚º: {dataset_info['total_size']}")

        if dataset_info.get('description'):
            print(f"\nğŸ“ ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
            print(f"  {dataset_info['description']}")

        if dataset_info.get('features'):
            print(f"\nğŸ”§ ä¸»è¦ãªç‰¹å¾´é‡ ({len(dataset_info['features'])}ä»¶):")
            for feature in dataset_info['features']:
                print(f"  - {feature}")
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±æŠ½å‡ºå¤±æ•—")

    print("\n" + "=" * 60)
    print("ãƒ†ã‚¹ãƒˆå®Œäº†")
    print("=" * 60)


if __name__ == "__main__":
    main()
